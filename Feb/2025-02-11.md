# TIL Template

## 날짜: 2025-02-11
### 스크럼
- 학습 목표 1 : 크램폴린 특강 듣기
- 학습 목표 2 : 데이터 시각화 정리

### 새로 배운 내용
#### 주제 1: 크램폴린 특강 듣기

# 컨테이너 기술 톺아보기

## 로컬, VM 기반의 배포

### 로컬

- 버전 충돌 문제 발생 가능성 높음

### VM

- VM을 여러개 띄어서 다른 앱에 영향을 안끼치게 만들기 → 격리된 구조로 만들기
- 무겁다는 문제점 발생, 리소스도 크게 든다
- 확장성의 문제에도 직면

## 컨테이너 기반의 배포

- 리눅스의 cgroups, namespace를 활용
- 명령어
    - FROM : base image를 빌드해서 사용
    - WORKDIR : 작업 디렉토리 설정
    - COPY : 소스 코드를 이미지에 복사
    - RUN : 프로젝트 빌드
    - ENV : 환경 변수 삽입
    - CMD :
- 장점
    - 확장성, 유연성이 좋다.
    - 각각의 서비스를 격리된 환경에서 배포
    - VM에 비해 더 가볍게 가상화를 할 수 있다.
- 컨테이너란?
    - 배포해야하는 어플리케이션, 환경을 컨테이너처럼 담는다라고 보면 된다.

## 쿠버네티스의 역할

- 컨테이너를 관리할 수 있도록 도와주는 툴
- 유연한 확장, 리소스 절약, 장애 대응 가능
- Auto Scaling : 컨테이너의 추상화된 배포를 이용하여 트래픽이 몰리는 시기를 효율적으로 처리
- Auto Healing : 서비스 운영 시에 장애 대응
    - 1대의 백업 서버를 두어 리소스 낭비를 대비
    - 컨테이너가 죽으면 새롭게 띄어서 대비
1. 유연한 확장을 가능하게 한다.
2. 다양한 배포 및 운영 기법을 자동화한다.

→ 유연한 특징을 최대한 활용해서 리소스를 절약하고 효율적인 서비스를 제공

## 쿠버네티스 기본 개념

- 클러스터 : 가장 큰 단위
    - 노드 : 서버
        - object : namespace → 노드에 종속되지 않음(추상화 됨)
            - 파드(Pod) : 쿠버네티스의 가장 작은 컴퓨팅 단위 → 실제 개발이 진행되는 곳
- 레플리카셋(ReplicaSet) : 다수의 Pod를 관리 → AutoScaler와 함께 사용하면 복제본 개수를 자동 조절
- 디플로이먼트(Deployment) : 파드를 새로운 버전으로 업데이트, 다양한 배포 방식 지원(Recreate, RollingUpdate)
- 서비스(Service): Pod 그룹에 트래픽을 분산해서 전달 → 로드밸런서 역할
- IaC : Infra as Code
    - 모든 쿠버네티스 오브젝트들은 코드로 작성된다.

---

# 크램폴린 IDE

## 크램폴린 IDE의 사용 플로우

- 크램폴린 IDE를 이용해 애플리케이션을 개발
    - 작업 공간이 생성
- Dockerfile 작성
    - 전체 프로젝트를 github에 올리기
- D2Hub(카카오에서 운영하는 원격 저장소) 레포지토리에 이미지를 빌드 및 업로드
- 애플리케이션 이미지 배포를 위한 yaml 파일을 작성
- Kargo(지속적인 배포 기능을 가진 도구, argocd기반 카카오 CI/CD 툴)를 통해 DKOS 클러스터에 D2HUB 이미지를 배포

### 전체 플로우 정리

1. 이미지 빌드 요청
2. 지정된 소스 저장소에서 코드를 가져와서 이미지 빌드
3. 배포 요청
4. yaml에 명시된 D2Hub 이미지 참고
5. 이미지를 이용해 DKOS Cluster에 배포
6. 배포된 리소스에 대한 외부 URL 등록 요청
7. 외부 URL 등록
8. 사용자가 외부 URL로 요청 시 DKOS Cluster의 앱으로 연결 됨

### 안내점

1. 커밋은 main 브랜치에 작성
2. github  소스 코드 상에 변경 사항이 발생하면 d2hub 재빌드 → kargo 재배포
3. 크램폴린 IDE 개발 환경은

#### 주제 2: 데이터 시각화 정리
## Seaborn

### 정의

- 통계적 데이터 시각화를 쉽게 구현할 수 있도록 고급 스타일과 기능을 제공하는 파이썬 데이터 시각화 라이브러리
- Matplotlib을 기반으로 하며, Pandas의 데이터프레임과 잘 호환되도록 설계되어 있어 복잡한 시각화도 간단한 코드로 표현할 수 있다.

### 특징

| 특징 | 설명 |
| --- | --- |
| **Matplotlib 기반의 고급 시각화 지원** | Seaborn은 Matplotlib 위에서 동작하며, **더 직관적이고 보기 좋은 스타일의 그래프**를 생성하도록 다양한 기능을 제공합니다.
Matplotlib을 직접 사용할 때보다 더 간결한 코드로 높은 수준의 시각화를 만들 수 있습니다. |
| **통계적 데이터 시각화 기능 제공** | Seaborn은 데이터를 단순히 시각화하는 것을 넘어, 통계적 특성을 강조하는 기능을 포함합니다.
예를 들어, 회귀선(regression line) 추가, 밀도 추정(density estimation), 범주형 데이터 표현 등 다양한 통계 분석을 지원합니다. |
| **Pandas 데이터프레임과의 높은 호환성** | Seaborn은 Pandas 데이터프레임을 직접 활용할 수 있도록 설계되었습니다.
따라서 Pandas 구조를 그대로 사용해 간단한 코드로도 복잡한 데이터 시각화를 쉽게 수행할 수 있습니다. |
| **다양한 스타일 및 테마 제공** | Seaborn은 기본적으로 고급 스타일이 적용된 그래프를 제공합니다.
또한, `set_style()`, `set_palette()` 등의 함수를 통해 그래프의 배경, 색상, 스타일 등을 손쉽게 변경할 수 있습니다. |

### 용어 정리

- 회귀선(Regression Line) : 두 변수 간의 관계를 수학적 모델로 표현한 직선으로, 데이터의 경향성을 나타낸다.
- 밀도 추정(Density Estimation) : 데이터의 분포를 부드러운 곡선으로 표현하는 기법으로, 히스토그램보다 연속적인 확률 분포를 보여준다.

### 사용 이유

- 데이터를 단순히 표현하는 데 그치지 않고, 변수 간 관계, 데이터의 분포, 그룹 간 비교, 통계적 트렌드 등을 직관적으로 파악하기 위해

| **이유** | **설명** |
| --- | --- |
| **통계적 데이터 시각화 최적화** | 단순 그래프가 아닌, 데이터의 통계적 특성을 반영한 시각화를 쉽게 구현할 수 있습니다.
예를 들어 회귀선(regression line), 밀도 추정(density estimation), 박스 플롯(box plot) 등 다양한 통계 그래프를 기본 제공합니다. |
| **기본적으로 세련된 스타일 제공** | Seaborn은 기본적으로 고급스럽고 가독성 높은 스타일을 적용합니다.
색상 팔레트, 축 스타일, 격자(grid) 등이 자동 조정되어 그래프의 가독성을 높여줍니다. |
| **Matplotlib보다 간결한 코드** | Matplotlib에서는 세부 설정이 많지만, Seaborn은 짧은 코드로도 효과적인 시각화를 구현할 수 있습니다. |
| **데이터프레임과의 강력한 연동** | Pandas의 DataFrame과 연동되어, 데이터를 직접 입력하여 시각화할 수 있습니다.
데이터프레임 컬럼을 지정해 그래프를 쉽게 생성할 수 있습니다. |
| **카테고리형 데이터 시각화 강점** | 그룹별 평균, 변동성, 밀도 분포 등 다양한 통계 기능을 제공하여 범주형 데이터를 직관적으로 비교할 수 있습니다. |
| **자동 축 조정 및 스타일 최적화** | Seaborn은 데이터를 기반으로 축 범위를 자동 조정하고, Matplotlib보다 깔끔한 기본 스타일을 제공합니다. |

---

## 범주형 데이터(Categorical Data)

### 정의

- 정해진 그룹이나 레이블을 가지는 데이터
- 명목형(Nominal) 또는 순서형(Ordinal) 변수로 구성

### 특성

| 특징 | 설명 |
| --- | --- |
| **이산적인 값(Discrete Values)** | 범주형 데이터는 연속적인 값이 아니라, 몇 가지로 정해진 값만 가질 수 있습니다.
- 예: "남성/여성", "승인/거부", "A/B/C/D 등급" 등 |
| **수학적 연산 불가능** | 범주형 데이터는 숫자로 표현될 수 있지만, 산술 연산(덧셈, 평균 계산 등)은 의미가 없다.
- 예: "A 등급(1)" + "B 등급(2)" = 의미 없음. |
| **명목형(Nominal)과
순서형(Ordinal)으로 구분** | **- 명목형(Nominal):** 순서가 없는 범주 데이터 (예: 성별, 혈액형, 국가)
- **순서형(Ordinal):** 일정한 순서를 가지는 범주 데이터 (예: 교육 수준, 만족도 조사) |
| **시각화 방식이 다름** | 범주형 데이터는 일반적으로 막대 그래프, 박스 플롯 등으로 표현됩니다. |

### 사용 이유

- 그룹 간 차이와 패턴을 직관적으로 비교 분석하기 위해
- 정해진 그룹이나 레이블을 가지는 데이터로, 숫자로 표현되는 연속형 데이터와 달리 명확하게 구분할 수 있다.
- 데이터의 패턴을 쉽게 파악하고, 그룹 간 차이를 보다 효과적으로 분석할 수 있다.
    
    
    | **이유** | **설명** |
    | --- | --- |
    | **그룹 간 차이 비교** | 범주별 데이터 크기와 차이를 시각적으로 쉽게 비교할 수 있습니다.
    (예: 제품 유형별 평균 매출) |
    | **데이터 분포 확인** | 각 범주의 데이터가 어떻게 분포하는지, 편향되거나 이상치(outlier)가 있는지 분석할 수 있습니다.
    (예: 성별에 따른 시험 점수 분포) |
    | **이상치(outlier) 탐지** | 박스 플롯과 같은 시각화 기법을 통해 특정 범주의 이상치를 손쉽게 식별할 수 있습니다. |
    | **패턴 및 트렌드 분석** | 시간에 따른 카테고리별 변화나 트렌드를 효과적으로 확인할 수 있습니다.
    (예: 월별 고객 유형별 구매량 변화) |
    | **데이터 기반 의사결정 지원** | 카테고리별 차이를 정량적으로 분석하여 비즈니스나 연구에서 데이터 기반 결정을 내리는 데 도움을 줍니다. |

### 알면 좋은 정보

- **Matplotlib vs Seaborn**
    - Matplotlib : 저수준 (low-level) 라이브러리로, 주로 “어떤 그래프를 그릴 것인가?”
    - Seaborn : 고수준(high-level) 라이브러리로, “어떤 데이터를 어떻게 시각화할 것인가?”
        - 함수명 자체가 데이터 유형 또는 관계와 밀접하게 연결되어 있음

---

## 연속형 데이터(Continuous Data)

### 정의

- 특정 구간 내에서 이론적으로 무한한 갑슬 가질 수 있는 데이터를 말한다.
- 즉, 두 데이터 값 사이 이론적으로 무한한 수의 중간 값이 존재할 수 있다.
- 연속형 데이터를 시각화할 수 있는 다양한 그래프를 제공하기 때문에 데이터의 분포, 패턴을 직관적으로 분석할 수 있다.

### 특징

| 특징 | 설명 |
| --- | --- |
| 수치적이며 연속된 값 | 연속형 데이터는 정해진 범위 내에서 실수 값을 가질 수 있으며, 소수점 단위까지 표현할 수 있습니다.
- 예: 키(175.3cm), 온도(23.6℃), 시간(12.45초), 연봉(5,530만 원) |
| **측정 단위에 따라 세분화 가능** | 특정한 측정 단위(소수점, 밀리미터, 밀리초 등)로 더 정밀하게 표현할 수 있습니다.
- 예: 거리(1.23km vs. 1.2345km), 무게(65.4kg vs. 65.432kg) |
| **연속적인 변화와 경향 분석 가능** | 시간에 따른 변화(시계열 데이터), 특정 변수의 증가/감소 경향성을 분석할 수 있습니다.
- 예: 주식 가격 변화, 기온 변화, 연령대별 평균 소득 증가율 |
| **Seaborn에서 다양한 형태의 시각화 가능** | 히스토그램(histplot), 커널 밀도 추정(kdeplot), 선 그래프(lineplot), 산점도(scatterplot) 등 다양한 방법으로 표현할 수 있습니다. |

### 빈도와 확률 밀도

| 구분 | 설명 |
| --- | --- |
| **빈도
(Frequency)** | 데이터를 특정 구간(bin)으로 나누고, 각 구간에 속하는 데이터 개수를 계산하는 방식입니다.
히스토그램(histplot)을 통해 시각화할 수 있습니다.
- `예:` 키 데이터를 5cm 단위로 구간을 나누고, 각 구간에 속하는 사람 수를 막대로 표현 |
| **확률 밀도
(KDE, Kernel Density Estimation)** | 빈도를 막대(bin) 형태로 표현하는 것이 아니라, 연속적인 확률 밀도 곡선을 생성하는 방식입니다.
특정 값 주변에 데이터가 얼마나 밀집되어 있는지 부드러운 곡선으로 나타내며, 커널 밀도 추정(kdeplot)을 사용하여 시각화할 수 있습니다.
- `예:` 키 데이터에서 170cm 근처에 가장 많은 사람들이 몰려 있다면, KDE 곡선이 170cm 부근에서 가장 높게 나타남 |

### 이유

- 수치적 관계를 정량적으로 분석하고 미래 변화를 예측하기 위해서
- 수치적으로 연속된 값을 가지므로 정량적 분석이 가능하며, 패턴을 탐색하거나 미래 변화를 예측할 때 주로 사용한다.
- 연속형 데이터 시각화는 데이터의 분포와 경향성을 효과적으로 파악
    
    
    | **이유** | **설명** |
    | --- | --- |
    | **변수 간 관계 분석** | 연속형 데이터로 여러 변수 간의 상관관계를 파악할 수 있습니다.
    예를 들어, 온도와 판매량, 학습 시간과 성적 등의 관계를 산점도 또는 회귀선을 통해 분석할 수 있습니다. |
    | **변화 및 추세 예측 가능** | 시간에 따른 연속적 변화를 분석해 미래 변동을 예측할 수 있습니다.
    예를 들어, 주식 시장 가격, 날씨 변화, 경제 성장률 등을 분석할 때 연속형 데이터를 사용합니다. |
    | **정량적 해석을 통한 의사결정 지원** | 연속형 데이터의 정량적 수치는 객관적인 데이터 기반 의사결정을 돕습니다.
    예를 들어, 평균 수익률을 분석하여 투자 전략을 수립하거나, 제품 판매 데이터를 분석하여 마케팅 전략을 수립할 수 있습니다. |

---

## 관계 데이터 (Relational data)

### 이유

- 두 개 이상의 변수 간의 상관관계나 패턴을 분석하는 데이터
- 수치형 데이터 간의 상관관계를 분석하는 데 활용되며, 산점도, 회귀선이 포함된 산점도, 페어 플롯 등 다양한 시각화 기법으로 표현
    
    
    | 예시 | 설명 |
    | --- | --- |
    | **경제 데이터** | 소비 증가가 매출에 미치는 영향 |
    | **건강 데이터** | 운동량과 체중 감소 간의 관계 |
    | **교육 데이터** | 학습 시간과 시험 점수 간의 상관관계 |
    | **환경 데이터** | 대기 오염도와 호흡기 질환 발생률 간의 관계 |

### 특징

| 특징 | 설명 |
| --- | --- |
| **변수 간의 관계 분석** | 두 개 이상의 변수를 비교하여 **한 변수의 변화가 다른 변수에 미치는 영향**을 분석할 수 있습니다. |
| **수치형 데이터 활용** | 관계 데이터는 주로 연속형 변수 간의 관계를 분석하며, 특정 패턴을 찾는 데 유용합니다. |
| **시각화를 통해 관계 탐색** | Seaborn의 다양한 그래프(산점도, 회귀선, 페어 플롯 등)를 활용해 변수 간 관계를 직관적으로 탐색할 수 있습니다. |
| **양적 관계와 질적 관계 분석 가능** | 두 변수 간의 **양의 상관관계, 음의 상관관계, 무상관**을 시각적으로 확인할 수 있습니다. |
| **다변량 분석 지원** | 여러 변수를 한 번에 비교하는 페어 플롯(Pair Plot)을 통해 다차원 관계를 한눈에 분석할 수 있습니다. |

### 핵심

- 한 변수의 변화가 다른 변수에 어떤 영향을 미치는지를 파악하는 것이다.
- 관계 데이터를 분석함으로써 변수 간의 패턴을 발견하고, 상관관계를 시각적으로 확인하며, 예측 모델을 구축하는 데 활용할 수 있다.

### 이유

- 변수 간의 관계를 파악하여 데이터 기반의 예측, 최적화, 의사 결정을 정확하고 효과적으로 하기 위해
- 특정 변수가 다른 변수에 미치는 영향, 변수 간 상관성, 데이터의 그룹화 가능성 등을 파악할 수 있다.
- 관계 데이터를 시각적으로 분석하면 숫자로만 확인했을 때 놓칠 수 있는 숨겨진 패턴을 발견할 수 있다.
    
    
    | **이유** | **설명** |
    | --- | --- |
    | **변수 간의 영향력 분석** | 한 변수가 다른 변수에 영향을 미치는지를 분석할 수 있습니다.
    예를 들어, 광고 비용이 매출 증가에 어떤 영향을 주는지 확인할 수 있습니다. |
    | **상관관계 파악** | 두 변수 간의 관계를 수치화하고 시각적으로 표현할 수 있습니다.
    예를 들어, 온도와 아이스크림 판매량 사이에 양의 상관관계가 있는지 확인할 수 있습니다. |
    | **패턴 및 트렌드 발견** | 변수 간의 관계를 통해 **숨겨진 패턴**을 찾고, 데이터의 변화 추이를 분석할 수 있습니다.
    예를 들어, 소득 수준과 소비 습관의 변화를 파악할 수 있습니다. |
    | **데이터 기반 의사 결정 지원** | 변수 간 관계를 분석하여 데이터에 근거한 의사 결정을 내릴 수 있습니다.
    예를 들어, 고객 만족도가 높은 제품이 매출에 **긍정적인** 영향을 미치는지 분석할 수 있습니다. |
    | **예측 모델 구축 및 최적화** | 상관관계가 높은 변수를 찾아 머신러닝 모델의 성능을 높이거나, 특정 변수를 조절하여 최적의 결과를 얻을 수 있습니다.
    예를 들어, 웹사이트 방문자의 행동 데이터를 기반으로 **최적의 광고 전략**을 수립할 수 있습니다. |

## 시계열 데이터(Time Series Data)

### 정의

- 시간의 흐름에 따라 일정한 간격으로 측정된 연속적인 데이터

### 이유

- 시간에 따라 변화를 분석하여 미래를 예측하고, 불확실성을 줄이기 위해

| **이유** | **설명** |
| --- | --- |
| **시간에 따른 변화 분석** | 과거부터 현재까지의 데이터를 통해 시간에 따른 변화를 파악할 수 있습니다.
예를 들어, 주식 시장의 변동성, 기온 변화, 트래픽 패턴 등을 분석할 수 있습니다. |
| **추세(Trend) 식별** | 장기적인 데이터 흐름을 분석해 증가·감소 추세나 주기적인 패턴을 파악할 수 있습니다. |
| **계절성(Seasonality) 분석** | 특정 시점마다 반복되는 패턴을 분석하여 계절별·주기적인 변동을 파악할 수 있습니다.
예를 들어, 전력 소비량은 여름과 겨울에 높고, 소매업 매출은 연말에 증가하는 경향이 있습니다. |
| **이상치(Outlier) 탐지** | 특정 시점에서 발생하는 급격한 변화(급등·급락)를 감지할 수 있습니다.
예를 들어, 금융 데이터에서의 주가 급락, 서버 로그 데이터에서의 비정상적인 접속량 증가 등을 탐지할 수 있습니다. |
| **이벤트 감지(Event Detection)** | 시간의 흐름에 따라 특정 사건(예: 자연재해, 소셜미디어 트렌드 급상승, 금융시장 충격)이 데이터에 미치는 영향을 분석할 수 있습니다. |

---

## 리샘플링

### 정의

- 시계열 데이터의 분석을 위해 데이터의 시간 간격을 재조정하여 다운샘플링(축소)하거나 업샘플링(확대)하는 과정
- 시계열 데이터는 원본 기록의 시간 간격이 불규칙하거나, 분석 목적에 따라 다른 시간 단위로 변환해야 할 때가 많다.
- 리샘플링을 통해 데이터의 시간 간격을 통합하거나, 세밀한 패턴을 관찰할 수 있도록 데이터를 보완할 수 있다.

### 주요 개념

| 개념 | 설명 |
| --- | --- |
| **다운샘플링(Downsampling)** | 기존 데이터보다 더 긴 시간 간격으로 데이터를 집계하는 과정입니다.
예를 들어, 초 단위 데이터를 분, 시간, 일 단위로 변환하는 것이 다운샘플링에 해당합니다.
평균, 합계, 최대값, 최소값, 중앙값 등의 집계 함수를 사용하여 데이터를 요약합니다. |
| **업샘플링(Upsampling)** | 기존 데이터보다 더 짧은 시간 간격으로 데이터를 확장하는 과정입니다.
예를 들어, 일 단위 데이터를 시간, 분, 초 단위로 변환하는 것이 업샘플링에 해당합니다.
보간(Interpolation) 기법을 사용하여 누락된 값을 보충할 수 있습니다. |
| **고정된 시간 간격으로 변환** | 리샘플링은 데이터의 시간 간격을 일정한 주기로 변경하는 것이므로, 데이터가 고정된 간격으로 정렬됩니다. |
| **집계(Aggregation)와
보간(Interpolation)** | - 다운샘플링에서는 시간 간격이 길어지므로, 평균·합계·최대값 등 집계(Aggregation) 연산이 필요합니다.
- 업샘플링에서는 시간 간격이 짧아지므로, 보간(Interpolation) 기법을 활용해 새로운 값을 생성합니다. |

### 사용 이유

- 불규칙한 시계열 데이터를 일정한 간격으로 정리하여 패턴과 트렌드를 명확하게 파악하기 위해서
- 데이터의 시간 간격을 조정하면 패턴과 트렌드를 명확하게 파악하고, 분석 및 시각화의 효율성을 극대화할 수 있다.
- 분석 목적에 따라 데이터의 해상도를 조정(업샘플링/다운샘플링)하여, 세부적인 변동성을 강조하거나 장기적인 추세를 부각할 수 있다

| 이유 | 설명 |
| --- | --- |
| **불규칙한 시간 간격을 일정하게 정리하기 위해** | 원본 시계열 데이터는 센서, 로그 데이터, 실험 결과 등에서 수집될 때 시간 간격이 일정하지 않을 수 있습니다.
리샘플링을 통해 일정한 간격으로 데이터를 변환하면 분석 및 시각화에 도움을 줍니다. |
| **시각적으로 명확한 패턴을 파악하기 위해** | 데이터가 너무 세밀하거나 불규칙하면 노이즈가 많아 시각적으로 흐릿한 패턴이 나타날 수 있습니다.
리샘플링을 통해 적절한 간격을 설정하면, 변동성을 줄이고 명확한 트렌드를 시각적으로 확인할 수 있다. |
| **과도한 변동성을 완화하고 노이즈를 제거하기 위해** | 초 단위 또는 밀리초 단위 데이터는 너무 세밀하여 불필요한 변동성이 많을 수 있습니다.
다운샘플링을 통해 불필요한 변동을 제거하고, 데이터의 전반적인 흐름을 더 직관적으로 분석할 수 있습니다. |
| **세부적인 변화를 강조하기 위해** | 데이터가 너무 거칠게 기록된 경우(예: 월별 데이터), 업샘플링과 보간(Interpolation)을 활용하면 보다 세밀한 변화를 분석할 수 있습니다.
작은 변화도 중요한 경우(예: 금융 시장 분석)에 유용합니다. |
| **비교 가능한 분석 단위를 만들기 위해** | 서로 다른 시간 간격으로 기록된 데이터를 비교해야 할 때, 동일한 단위(예: 주별, 월별)로 변환하면 일관된 분석이 가능합니다.
예를 들어, A사의 매출 데이터는 일 단위, B사는 주 단위로 기록되었다면, 동일한 단위로 맞추는 것이 가능합니다. |
| **모델 학습 및 예측을 위해 데이터 준비** | 머신러닝 모델이나 통계적 분석을 수행할 때, 일정한 시간 간격을 유지하는 것이 중요합니다.
리샘플링을 통해 모델 학습에 적합한 형태로 데이터를 정리할 수 있습니다. |

<aside>
💡

리샘플링은 단순히 시간 간격을 변경하는 것이 아니라, 데이터의 패턴과 트렌드를 강조하고 해석의 명확성을 높이는 과정

데이터를 분석 목적에 맞게 변환하여 의미 있는 인사이트를 도출하고, 더 효과적인 데이터 시각화를 구현할 수 있음

</aside>

<aside>
💡

그래프 결과만 보면 업샘플링이 원본 데이터랑 거의 동일하고 다운샘플링은 모양이 완전 달라지는데, 업샘플링이 더 나은거 아닌가요?

→ 업샘플링은 기존 데이터의 패턴을 유지하면서 더 촘촘한 간격으로 변환되므로 원본과 유사해 보인다.

하지만 보간법을 사용해 새 값을 생성하는 과정이므로 **실제 데이터가 아니라 추정된 값**이 포함된다.

반면 다운샘플링은 데이터 손실이 발생하지만 **노이즈를 제거하고 전반적인 패턴을 강조**하는 효과가 있다.

즉, **단기 변동이 중요한 경우 업샘플링이 유용하지만, 장기적인 패턴을 분석할 때는 다운샘플링이 더 적절합니다.**

</aside>

### 질문 (scale vs sample)

scale : 기하학적 

sample : 통계적 / 모집단 측면에서

## 이동평균(Moving Average)

### 정의

- 시계열 데이터의 변동성을 완화하고 장기적인 추세를 파악하기 위해 일정 구간의 평균값을 계산하는 기법
- 고정된 크기의 윈도우를 설정하여 해당 구간 내의 평균값을 계산하는 방식

### 개념

| 개념 | 설명 |
| --- | --- |
| **고정된 구간(Window) 내에서 평균 계산** | 이동평균은 특정 개수의 데이터 포인트를 하나의 그룹으로 묶어 평균을 계산합니다.
이때 사용되는 구간의 크기를 **윈도우 크기(Window Size)** 라고 합니다.
예를 들어, `윈도우 크기=3`인 이동평균은 현재 값과 이전 두 개의 값을 평균 내어 새로운 값을 생성합니다. |
| **시간에 따라 평균이 이동하며 새로운 값 생성** | 원본 데이터의 끝까지 이동하면서 평균값을 갱신합니다.
각 시점에서의 평균값을 기반으로 새로운 시계열 데이터를 생성하므로, 연속된 구간별 변동 추세를 확인할 수 있습니다. |
| **단기 변동(Noise) 완화** | 이동평균을 적용하면 시계열 데이터의 급격한 변동이 줄어들며, 보다 부드러운 곡선이 나타납니다. |

### 종류

1. 단순 이동평균(SMA, Simple Moving Average)
    - 단순 이동평균은 윈도우 내 모든 값을 동일한 가중치로 평균을 계산
    - **예시 (윈도우 크기=3)**
        
        ```python
        # 데이터: 10, 20, 30, 40, 50
        첫 번째 이동평균: (10 + 20 + 30) / 3 = 20.0
        두 번째 이동평균: (20 + 30 + 40) / 3 = 30.0
        세 번째 이동평균: (30 + 40 + 50) / 3 = 40.0
        ```
        
    - **특징**
        - 계산이 직관적이고 단순
        - 최근 데이터의 변화를 신속하게 반영하지 못할 수 있다.
        - 장기적인 흐름을 분석하는 데 적합하다
2. 지수 이동평균
    - 최근 데이터에 더 높은 가중치를 부여하여, 변동에 더 민감하게 반응하는 방식
    - **예시 (윈도우 크기=3, 가중치 적용)**
        
        ```python
        # 데이터: 10, 20, 30, 40, 50
        첫 번째 EMA: 10.0 (초기값)
        두 번째 EMA: (20 * α) + (10 * (1 - α)) = 15.0
        세 번째 EMA: (30 * α) + (15 * (1 - α)) = 22.5
        네 번째 EMA: (40 * α) + (22.5 * (1 - α)) = 31.3
        다섯 번째 EMA: (50 * α) + (31.3 * (1 - α)) = 40.6
        ```
        
        (여기서 `α`는 가중치 계수로, 일반적으로 `α = 2 / (윈도우 크기 + 1)`으로 계산합니다.)
        
    - 특징
        - 최근 데이터의 변화를 빠르게 반영
        - 노이즈를 줄이면서도 신속한 반응이 필요한 경우 유용하다
        - 금융 시장에서 주가 변동 분석 등에 자주 사용된다.
3. 가중 이동평균 (WMA, Weighted Moving Average)
    - 가중 이동평균은 최근 값에 더 큰 가중치를 부여하되, 지수 이동평균보다 선형적인 바익으로 가중치를 적용하는 방법
    - 예시(윈도우 크기 = 3, 가중치 적용)
        
        ```python
        # 데이터: 10, 20, 30, 40, 50
        첫 번째 WMA: (10 * 0.1) + (20 * 0.3) + (30 * 0.6) = 25.0
        두 번째 WMA: (20 * 0.1) + (30 * 0.3) + (40 * 0.6) = 36.0
        세 번째 WMA: (30 * 0.1) + (40 * 0.3) + (50 * 0.6) = 46.0
        ```
        
    - 특징
        - 단순 이동평균보다 최근 데이터의 가중치를 높여 빠른 반응을 제공
        - 지수 이동평균보다 선형적으로 변동을 반영
        - 특정 구간내에서 가중치를 조절하며 트렌드를 분석할 때 유용하다

| 이동평균 유형 | 계산 방식 | 특징 | 활용 예시 |
| --- | --- | --- | --- |
| **단순 이동평균 (SMA)** | 동일한 가중치로 평균 계산 | 계산이 단순하지만 최근 데이터 반영이 느림 | 장기적인 트렌드 분석 |
| **지수 이동평균 (EMA)** | 최근 값에 더 높은 가중치 부여 | 변동성을 빠르게 반영 가능 | 금융 시장, 주가 분석 |
| **가중 이동평균 (WMA)** | 가중치를 선형적으로 적용 | 특정 구간 내에서 가중치를 조절 가능 | 트렌드 분석, 주식 가격 예측 |

### 사용 이유

- 시계열 데이터에서 변동성을 줄이고, 장기적인 패턴과 트렌드를 명확하게 분석하기 위해서 사용
- 원본 데이터는 단기 요인에 의해 급격한 변동이 발생하기 쉬워, 흐름을 파악하기 어렵기 때문이다.

---

## 금융 데이터(Financial Data)

### 정의

- 데이터 시각화에서 금융 데이터는 주식 가격, 거래량, 환율, 금리 등 금융 시장에서 발생하는 시계열 기반의 데이터를 의미
- 시간의 흐름에 따라 변동하는 연속형 데이터

### 구성 요소

| **구성 요소** | **설명** |
| --- | --- |
| **주식 가격 데이터
(Stock Prices)** | 주식의 시가(Open), 고가(High), 저가(Low), 종가(Close), 조정 종가(Adjusted Close)로 구성됩니다.
시가는 장 시작 시 최초 가격, 고가는 당일 최고 가격, 저가는 당일 최저 가격, 종가는 장 종료 시 가격을 의미합니다.
조정 종가는 배당 및 주식 분할을 반영해 장기 분석에 활용됩니다. |
| **거래량 데이터
(Trading Volume)** | 일정 기간 동안 거래된 주식의 총 수량을 나타냅니다.
거래량이 많을수록 해당 주식이 활발히 거래되어 시장 관심도가 높음을 의미합니다. |
| **지수 데이터
(Market Index)** | S&P 500, NASDAQ, KOSPI 등 주식 시장 지수를 의미합니다.
여러 주식의 가중 평균으로 산출되며, 시장 전체 흐름을 보여줍니다. |
| **환율 데이터
(Foreign Exchange, Forex)** | 두 국가 통화 간 교환 비율을 의미합니다.
국제 무역, 경제 정책, 중앙은행 결정 등에 따라 변동성이 높습니다. |
| **금리 데이터
(Interest Rate)** | 중앙은행이 결정하는 기준 금리와 채권 시장 금리를 포함합니다.
금리는 경제 성장과 인플레이션에 직접적인 영향을 주며, 대출 비용과 투자 수익률을 좌우하는 핵심 요소입니다. |
| **파생상품 데이터
(Derivatives Data)** | 선물(Futures), 옵션(Options), 스왑(Swaps) 등 기초 자산(주식, 원자재, 환율 등)의 가격 변동을 기반으로 하는 금융상품 데이터를 포함합니다. |
| **경제 지표
(Economic Indicators)** | GDP 성장률, 실업률, 소비자물가지수(CPI) 등 거시경제 데이터를 포함합니다.
경제 전반의 건강 상태를 평가하고, 금융 시장의 장기 방향을 예측하는 데 사용됩니다. |

### 수집 주기

| **유형** | **설명** |
| --- | --- |
| **틱(Tick) 데이터** | 초 단위 이하로 발생하는 모든 거래 체결 정보를 기록합니다. |
| **분봉(Minute Bar)** | 분(minute) 단위로 시가, 고가, 저가, 종가, 거래량 등을 집계한 데이터입니다. |
| **일봉(Daily Bar)** | 하루 단위로 시가, 고가, 저가, 종가, 거래량 등을 반영하는 가장 일반적인 형태입니다. |
| **주봉(Weekly Bar)** | 한 주(週) 단위로 가격과 거래량 등을 집계합니다. |
| **월봉(Monthly Bar)** | 한 달(월) 단위로 가격과 거래량 등을 집계합니다. |

### 사용 이유

- 자본의 흐름을 분석하여 경제 활동을 예측하고 최적의 의사결정을 내리기 위해서

| **이유** | **설명** |
| --- | --- |
| **시장 분석 및 예측** | 금융 데이터를 분석하면 주식, 채권, 외환 시장의 방향성을 예측할 수 있습니다.
가격 변동, 거래량, 경제 지표 등을 종합적으로 고려해 상승·하락 신호를 파악하고, 이를 바탕으로 매수·매도 시점을 결정할 수 있습니다. |
| **리스크 관리 및 헤지 전략 수립** | 금융 시장은 예측이 어려운 변동성을 지니므로, 금융 데이터를 통해 리스크를 분석·관리할 수 있습니다.
예를 들어, **파생상품(선물·옵션) 데이터를 활용해 환율·금리 변동 위험을 헤지**하고, 포트폴리오 변동성을 줄이는 전략을 세울 수 있습니다. |
| **기업 실적 평가 및 재무 분석** | 개별 기업의 주가, 매출, 영업이익, 부채비율 등 주요 데이터를 분석함으로써 기업의 재무 상태를 평가할 수 있습니다. |

### 금융 데이터 시각화

- mplfinance 라이브러리를 활용하면 캔들 차트와 추가 지표를 손쉽게 시각화할 수 있다.
- mplfinance는 금융 데이터 시각화를 위한 파이썬 라이브러리로 matplotlib을 기반으로 작동
- 특히, 캔들 차트, 거래량 바 차트, 추가 지표 등을 손쉽게 시각화할 수 있다.
    
    
    | 주요 특징 | 설명 |
    | --- | --- |
    | **캔들 차트 지원** | 금융 데이터(OHLC)를 기반으로 캔들 차트를 생성할 수 있습니다. |
    | **추가 지표 표시 가능** | `make_addplot()`을 사용해 이동평균, 변동성 등 추가 지표를 함께 시각화할 수 있습니다. |
    | **다양한 스타일 제공** | `yahoo`, `charles`, `classic` 등 다양한 스타일을 활용하여 차트를 꾸밀 수 있습니다. |
    | **거래량과 서브플롯 지원** | 가격 차트와 거래량을 함께 표시하거나, 보조 지표를 별도 패널로 분리할 수 있습니다. |
    | **matplotlib과 호환 가능** | 기본 `matplotlib` 기능과 조합하여 더욱 정밀한 커스터마이징이 가능합니다. |

### 용어 정리

- **캔들 차트란?**
    - 금융 데이터의 시가, 고가, 저가, 종가 정보를 시각적으로 표현하는 차트
    - 각 캔들은 몸통과 꼬리로 구성되며, 상승은 보통 초록색, 하락은 빨간색으로 표시된다.
- 캔들 차트 vs 박스플롯
    - 캔들 차트는 시간에 따른 금융 데이터(시가, 고가, 저가, 종가)를 나타내는 데 사용되며, 개별 캔들이 시간 순서대로 연결됩니다.
    - 반면, 박스플롯은 데이터 분포(중앙값, 사분위수, 이상치)를 분석하는 통계적 차트로, 시간 개념 없이 그룹별 데이터를 비교하는 데 사용됩니다.
- 변동성을 별도 패널에 표시하는 이유
    - 가격과 변동성은 크기의 기준이 다르기 때문에 한 그래프에 겹치면 어느 한쪽이 가려지거나 왜곡될 수 있습니다.
    - 변동성은 가격 수준보다 움직임의 폭(강도)을 중점적으로 보는 지표입니다.
    - 따라서 가격 차트와 분리하면 서로의 흐름을 독립적으로 파악하기 쉬워집니다

---

## SciPy(Scientific Python)

### 정의

- 과학 계산과 통계 분석을 위한 고급 수학 함수와 알고리즘을 제공하는 파이썬 라이브러리
- Numpy를 기반으로 구축되었으며, 다양한 수학적 함수와 알고리즘을 제공하여 과학, 공학, 데이터 분석 분야에서 폭넓게 활용
- SciPy는 단순한 수학 연산을 넘어, 최적화, 보간(interpolation), 선형 대수, 신호 처리, 확률 분포 및 통계 분석, 미적분 계산, FFT 변환 등과 같은 고급 과학 계산 기능을 지원합니다.

### 특징

| **특징** | **설명** |
| --- | --- |
| **NumPy 기반의 확장성** | SciPy는 NumPy 배열을 기반으로 동작하며, NumPy의 연산을 더욱 확장하여 다양한 과학 계산을 지원합니다. |
| **고급 수학 및 최적화 기능 제공** | 함수 최적화, 최소제곱법, 회귀 분석 등 수학적 최적화 기능을 제공합니다. |
| **신호 처리 기능 포함** | 푸리에 변환(FFT), 필터링, 웨이브렛 변환 등 신호 처리 기능을 제공합니다. |
| **통계 및 확률 분포 분석 지원** | 다양한 확률 분포, 샘플링, 가설 검정, 회귀 분석 기능이 포함되어 있습니다. |
| **선형 대수 연산 강화** | 희소 행렬 연산, 고유값 분해, LU 분해 등 선형 대수 기능을 제공합니다. |
| **보간 및 회귀 분석 지원** | 데이터 사이 값을 예측하는 보간(interpolation) 및 다항 회귀 분석 기능이 포함됩니다. |
| **미적분 및 적분 연산 가능** | 수치 적분(quad, dblquad 등) 및 미분 방정식(ODE) 해결 기능을 제공합니다. |

### 모듈 구성

| **모듈** | **설명** |
| --- | --- |
| **`scipy.optimize`** | 함수 최적화 및 최소화, 비선형 방정식 해 찾기 기능을 포함합니다. |
| **`scipy.stats`** | 확률 분포, 통계 분석, 가설 검정 및 확률 밀도 함수(PDF, CDF) 등을 제공합니다. |
| **`scipy.linalg`** | 선형 대수 연산을 수행하며, NumPy의 `numpy.linalg`보다 더 확장된 기능을 제공합니다. |
| **`scipy.fft`** | 빠른 푸리에 변환(FFT) 기능을 제공하여 신호 및 주파수 분석에 사용됩니다. |
| **`scipy.integrate`** | 수치 적분 및 미분 방정식(ODE) 해결 기능을 포함합니다. |
| **`scipy.interpolate`** | 데이터 보간(interpolation) 기능을 제공하여 연속적인 값 추정에 사용됩니다. |
| **`scipy.spatial`** | 공간 데이터 처리 기능을 포함하며, 최근접 이웃 검색(KDTree), 거리 계산 등을 제공합니다. |
| **`scipy.signal`** | 신호 처리 관련 기능(필터링, 컨볼루션, 주파수 분석 등)을 제공합니다. |
| **`scipy.sparse`** | 희소 행렬(Sparse Matrix) 연산 기능을 포함하여 대규모 행렬 연산을 최적화합니다. |
| **`scipy.ndimage`** | 이미지 처리 기능을 제공하며, 다양한 필터링 및 변환 기능을 포함합니다. |

### Scipy와 Numpy의 차이점

| **비교** | **NumPy** | **SciPy** |
| --- | --- | --- |
| **기능 초점** | 기본적인 배열 연산 및 선형 대수 연산 수행 | 고급 과학 계산, 최적화, 신호 처리, 통계 분석 제공 |
| **배열 연산** | 다차원 배열(ndarray) 연산, 브로드캐스팅 지원 | NumPy 배열을 기반으로 고급 수학 연산 확장 |
| **선형 대수** | `numpy.linalg` 모듈을 통해 기본적인 선형 대수 연산 제공 | `scipy.linalg`을 통해 더 강력한 선형 대수 연산 지원 |
| **최적화 및 회귀 분석** | 없음 | `scipy.optimize`를 통해 비선형 최적화, 최소제곱법 등 지원 |
| **신호 처리** | 없음 | `scipy.signal`을 통해 필터링, 컨볼루션, FFT 변환 지원 |
| **통계 분석** | 기초적인 통계 기능(`mean, median, std` 등) 제공 | `scipy.stats`를 통해 확률 분포, 가설 검정, 통계 분석 제공 |
| **보간 기능** | 없음 | `scipy.interpolate`를 통해 선형 및 다항식 보간 제공 |
| **적분 및 미분 방정식** | 없음 | `scipy.integrate`를 통해 수치 적분 및 미분 방정식 해석 지원 |

### 사용 이유

- 데이터 시각화에서 정교한 수학적 연산을 수행하여 데이터를 정제하고 의미를 극대화하기 위해서
- 단순히 그래프를 그리는 것이 아니라, 데이터의 의미를 효과적으로 전달하는 과정
- 실제 데이터는 노이즈, 결측값, 복잡한 패턴, 왜곡된 형태를 가질 수 이으며, 이를 제대로 표현하지 않으면 잘못된 해석을 초래

## 정규 분포(Normal Distribution)의 정의

### 정의

- 데이터가 평균을 중심으로 좌우 대칭을 이루며 종형 곡선을 따르는 확률 분포

### 용어 정리

- 확률 분포란?
    - 확률 분포는 어떤 확률 변수가 취할 수 있는 모든 값과 그 값이 발생할 확률을 나타내는 함수

### 정규 분포의 확률 밀도 함수

```python
f(x) = (1 / (σ * sqrt(2 * π))) * exp(- (x - μ)² / (2 * σ²))
```

| 구분 | 설명 |
| --- | --- |
| `μ` (mu) = **평균(Mean)** | 데이터의 중심 위치를 결정 |
| `σ` (sigma) = **표준 편차(Standard Deviation)**  | 데이터가 퍼져 있는 정도를 결정 |
| `π` (pi) | 약 3.14159 |
| `exp()` | 자연로그의 밑 `e`를 지수로 하는 함수 |
- 평균이 0이면 정규 분포의 중심이 0이 된다.
- 표준 편차가 작으면 분포가 좁고 뾰족해지고, 크면 넓고 완만해집니다.

### 특징

| **특징** | **설명** |
| --- | --- |
| **평균(μ)을 중심으로 좌우 대칭** | 데이터가 평균을 기준으로 대칭적으로 분포하며, 평균보다 작은 값과 큰 값이 동일한 확률을 가집니다. |
| **데이터의 68-95-99.7 법칙** | - 약 `68%`의 데이터가 `μ ± 1σ` 범위 내에 위치합니다.
- 약 `95%`의 데이터가 `μ ± 2σ` 범위 내에 위치합니다.
- 약 `99.7%`의 데이터가 `μ ± 3σ` 범위 내에 위치합니다. |
| **표준 편차(σ)에 따라 분포 형태가 결정됨** | 표준 편차가 작으면 분포가 좁고 뾰족해지며, 크면 넓고 완만해집니다. |
| **데이터가 평균 근처에 집중됨** | 평균에서 멀어질수록 확률이 급격히 감소하며, 극단적인 값이 발생할 가능성이 낮아집니다. |
| **여러 통계 기법에서 기본 가정으로 사용됨** | 가설 검정, 회귀 분석, 주성분 분석(PCA) 등에서 데이터가 정규 분포를 따른다고 가정하는 경우가 많습니다. |

### 표준 정규 분포(Standard Normal Distribution)와 Z-점수 변환

- 정규 분포를 표준화하면 평균이 0, 표준 편차가 1인 분포가 되며, 이를 표준 정규 분포라고 한다.
- 표준 정규 분포에서는 Z-점수 (Z-Score) 변환을 통해 원본 데이터를 표준화

**Z-점수 변환 공식**

```python
Z = (X - μ) / σ
```

| 구분 | 설명 |
| --- | --- |
| `X` | 원본 데이터 값 |
| `μ` | 평균 |
| `σ` | 표준 편차 |
| `Z` | 변환된 표준 점수 |
- Z-점수는 특정 값 X가 평균에서 몇 개의 표준 편차만큼 떨어져 있는지를 나타낸다.
- 서로 다른 정규 분포를 동일한 기준으로 비교할 수 있습니다.

### 정규 분포를 따르는 데이터 예시

| **예제 데이터** | **설명** |
| --- | --- |
| **사람의 키** | 특정 집단의 키는 평균을 중심으로 정규 분포를 따릅니다. |
| **시험 점수** | SAT, TOEFL, IQ 테스트 등은 정규 분포를 기반으로 점수를 설정합니다. |
| **기계 공정 오차** | 공장에서 제조된 제품의 크기 편차는 정규 분포를 따르는 경우가 많습니다. |
| **주식 시장 수익률** | 단기적인 주가 변동은 정규 분포에 가깝게 나타납니다. |

### 사용 이유

- 많은 데이터가 정규 분포를 따르며 분석과 예측을 단순하고 정확하게 할 수 있기 때문이다.
- 특히, 정규 분포는 평균과 표준 편차만으로 데이터의 전체적인 특성을 설명할 수 있어 간단하고, 통계 및 머신러닝에서 중요한 가정을 이루기 때문에 널리 사용

| **이유** | **설명** |
| --- | --- |
| **자연현상과 사회적 데이터가 정규 분포를 따름** | 많은 자연현상과 사회적 데이터(예: 사람들의 키, 체온, 시험 점수, 공정 오차, 금융 시장 변동 등)가 정규 분포를 따르거나 근사합니다.
따라서 정규 분포를 사용하면 현실 데이터를 더 정확하게 설명하고 분석할 수 있습니다. |
| **중심 극한 정리 (Central Limit Theorem, CLT)** | 개별 데이터가 정규 분포를 따르지 않더라도, 여러 요인이 결합되면 평균이 정규 분포를 따르는 경향이 있습니다. 
즉, 표본 크기가 충분히 크면 대부분의 데이터는 정규 분포에 가까워지며, 이를 통해 데이터 분석이 더 신뢰성 있게 수행될 수 있습니다. |
| **데이터의 특성을 평균과 표준 편차만으로 설명 가능** | 정규 분포는 평균(μ)과 표준 편차(σ)만 알면 전체적인 데이터 분포를 쉽게 이해할 수 있습니다. |
| **통계 분석과 머신러닝에서 기본 가정으로 사용** | t-검정, ANOVA, 회귀 분석, 주성분 분석(PCA) 등 대부분의 통계 기법이 정규 분포를 가정합니다.
따라서 데이터를 정규화하면 이러한 분석 기법을 더 신뢰성 있게 적용할 수 있으며, 결과의 정확성을 높일 수 있습니다. |
| **Z-점수를 활용한 데이터 비교 가능** | 서로 다른 정규 분포를 표준 정규 분포(Z-변환)로 변환하면 데이터 간 직접적인 비교가 가능해집니다. |

## 기술 통계(Descriptive Statistics)

### 정의

- 데이터 요약 정리하여 중앙 경향성, 산포도, 분포 형태 등의 지표를 계산하고 시각적으로 표현하는 분석 기법이다.
- 복잡한 데이터를 수치화해 한눈에 파악할 수 있도록 도와주며, 데이터의 패턴과 분포를 이해하고 이상값을 식별하는 데 유용하다.

| **구성 요소** | **설명** |
| --- | --- |
| **중앙 경향성 (Central Tendency)** | 데이터의 중심값을 나타내는 지표로, 평균(Mean), 중앙값(Median), 최빈값(Mode) 등이 포함됩니다. |
| **산포도 (Dispersion)** | 데이터 값들이 중심값을 기준으로 얼마나 퍼져 있는지를 나타내며, 분산(Variance), 표준 편차(Standard Deviation), 범위(Range), 사분위수(IQR) 등을 포함합니다. |
| **분포 특성 (Distribution Characteristics)** | 데이터 전체의 형태를 분석하는 지표로, 왜도(Skewness)와 첨도(Kurtosis)를 통해 분포의 대칭성과 뾰족함 정도를 파악합니다. |

### 핵심 지표

1. 중앙 경향성 (Central Tendency)

| 구분 | 설명 |
| --- | --- |
| **평균(Mean)** | 데이터 값의 합을 데이터 개수로 나눈 값으로, 대표적인 중심 경향성 지표입니다. |
| **중앙값(Median)** | 데이터를 정렬했을 때 가운데 오는 값으로, 이상값(Outlier)의 영향을 덜 받습니다. |
| **최빈값(Mode)** | 데이터에서 가장 빈도수가 높은 값으로, 자료의 대표적인 특성을 보여줍니다. |
1. 산포도 (Dispersion)

| 구분 | 설명 |
| --- | --- |
| **범위(Range)** | 최댓값과 최솟값의 차이를 통해 자료가 퍼져 있는 정도를 간단히 확인합니다. |
| **분산(Variance)** | 데이터 값들이 평균에서 얼마나 떨어져 있는지를 수치화한 지표로, 값이 클수록 변동성이 큽니다. |
| **표준 편차(Standard Deviation)** | 분산의 제곱근으로, 데이터의 평균적 변동성을 직관적으로 파악하는 데 쓰입니다. |
| **사분위 범위(IQR)** | 1사분위수(Q1)와 3사분위수(Q3)의 차이로, 데이터 중간 영역의 퍼짐 정도를 나타냅니다. |
1. 분포 특성 (Distribution Characteristics)

| 구분 | 설명 |
| --- | --- |
| **왜도(Skewness)** | 분포의 비대칭성을 나타내는 값입니다.
0보다 크면 오른쪽으로 치우친 분포(양의 왜도), 0보다 작으면 왼쪽으로 치우친 분포(음의 왜도)를 뜻합니다. |
| **첨도(Kurtosis)** | 분포가 뾰족한 정도로, 3보다 크면 정규 분포보다 뾰족한 분포(급첨), 3보다 작으면 평평한 분포(완만)를 의미합니다. |

### 이유

- 복잡한 데이터를 정리하고 요약하여 쉽게 이해할 수 있도록 하기 위해서

| **이유** | **설명** |
| --- | --- |
| **데이터를 요약하여 쉽게 이해할 수 있도록 함** | 원시 데이터는 방대한 숫자로 구성되어 전체적인 특징을 파악하기 어렵습니다.
기술 통계를 통해 평균·중앙값·표준 편차 등으로 핵심 정보를 한눈에 확인할 수 있습니다. |
| **데이터의 분포와 패턴을 파악할 수 있음** | 데이터가 정규 분포를 따르는지, 왜도가 있는지, 첨도가 높은지 등을 분석해 구조를 파악하고 적절한 분석 방법을 선택할 수 있습니다. |
| **이상값(Outlier) 탐지 및 데이터 정제 가능** | 기술 통계를 활용하면 평균에서 벗어난 이상값을 쉽게 찾아 제거·수정해 분석 왜곡을 방지할 수 있습니다. |
| **데이터 변동성과 신뢰도를 평가할 수 있음** | 표준 편차와 분산을 통해 데이터의 변동성과 일관성을 평가하고, 실험 데이터의 신뢰도를 확인할 수 있습니다. |
| **데이터 비교 및 그룹 간 차이 분석 가능** | 서로 다른 집단(A/B 테스트, 지역별 매출 등)의 평균·분산을 비교해 그룹 간 차이를 분석할 수 있습니다. |

## 가설 검정(Hypothesis Testing)

### 정의

- 가설 검정은 표본을 기반으로 통계적 가설의 참과 거짓을 검정하여 결론을 도출하는 과정이라는 의미이다.

### 가설 검정 vs 가설 검증

- 가설 검정 : 통계적 방법을 이용하여 가설이 맞는지 아닌지를 판단하는 과정
- 가설 검증 : 과학적 방법을 통해 가설이 참이 거짓인지를 입증하는 과정

### 핵심 개념

| **개념** | **설명** | **그래프에서의 위치** |
| --- | --- | --- |
| **귀무가설 (H₀)** | 귀무가설은 기본적으로 참이라고 가정하는 가설로, 연구자가 반증하고자 하는 대상입니다.
예를 들어, "새로운 치료법은 기존 치료법과 효과 차이가 없다"라는 가정이 이에 해당합니다.
쉽게 말해, "특별한 변화가 없다"고 보는 것입니다. | 검은색 곡선 (Null Distribution, H₀ is True)으로 표시되어 있으며,
귀무가설이 참일 때의 검정 통계량 분포를 나타냅니다. |
| **대립가설 (H₁)** | 대립가설은 검정하고자 하는 가설로, 귀무가설과 반대되는 주장입니다.
예를 들어, "새로운 치료법이 기존 치료법보다 효과적이다"라고 주장하는 것이 대립가설입니다.
즉, "변화가 있다" 또는 "차이가 있다"고 보는 것입니다. | 회색 점선 곡선 (Alternative Distribution, H₁ is True)으로 표시되어 있으며,
대립가설이 참일 때의 검정 통계량 분포를 나타냅니다. |
| **1종 오류 (Type I Error)** | 1종 오류는 실제로 귀무가설이 참인데, 이를 잘못 기각하는 오류입니다.
쉽게 말해, "효과가 없는데 있다고 착각하는 실수"입니다.
예를 들어, "실제로 약이 효과가 없는데 효과가 있다고 결론을 내리는 경우"입니다. | 빨간색 음영 영역 (Rejection Region, α)으로 표시되어 있으며,
귀무가설이 참일 때 검정 통계량이 이 영역에 속하면 기각됩니다. |
| **2종 오류 (Type II Error)** | 2종 오류는 실제로 대립가설이 참인데, 귀무가설을 기각하지 않는 오류입니다.
쉽게 말해, "효과가 있는데 없다고 판단하는 실수"입니다.
예를 들어, "실제로 약이 효과가 있는데 효과가 없다고 결론을 내리는 경우"입니다. | 파란색 음영 영역 (Type II Error, β)으로 표시되어 있으며,
대립가설이 참일 때 검정 통계량이 이 영역에 속하면 기각되지 않습니다. |

<aside>
❓

그래프에서 관찰값과 검정 통계량 둘 다 초록색 점선이면 둘이 같은 건가요?

</aside>

<aside>
👨‍💻

아닙니다.

그래프에서 초록색 점선은 **관찰값을 이용해 계산된 검정 통계량**을 나타냅니다.

즉, 관찰값 자체가 아니라, 이를 바탕으로 가설 검정을 수행한 결과입니다.

관찰값과 검정 통계량은 개념적으로 다르지만, 최종적으로 하나의 숫자로 표현될 수 있습니다.

따라서 초록색 점선이 가리키는 값은 **검정 통계량**이며, 관찰값 그 자체는 아닙니다.

</aside>

### **일반적인 절차**

- 가설 검정은 데이터를 분석하여 의미 있는 차이가 있는지를 평가하는 과정입니다.

| 순서 | **단계** | **설명** | **예시** |
| --- | --- | --- | --- |
| 1 | **가설 설정** | 귀무가설(H₀)과 대립가설(H₁)을 명확히 정의합니다. | H₀: "새로운 광고 전략은 기존과 차이가 없다."
H₁: "새로운 광고 전략이 더 효과적이다." |
| 2 | **유의수준(α) 결정** | 가설을 기각할 기준을 설정합니다. 일반적으로 0.05(5%)를 사용합니다. | α = 0.05 → "5% 확률까지는 우연으로 인정하겠다." |
| 3 | **검정 통계량 계산** | 적절한 검정 방법을 선택하고 검정 통계량을 계산합니다. | 평균 비교 → t-검정
분포 비교 → 카이제곱 검정 |
| 4 | **p값 계산 및 해석** | 검정 통계량을 이용해 p값을 구하고 해석합니다. | p값 < 0.05 → 귀무가설 기각
p값 ≥ 0.05 → 귀무가설 유지 |
| 5 | **결론 도출** | 가설 검정 결과를 해석하고 최종 결론을 내립니다. | "새 광고 전략이 유의미하게 효과적이다."
또는 
"차이가 통계적으로 유의하지 않다." |

### 유형

- 가설 검정에는 다양한 유형이 있으며, 비교하는 데이터의 성격과 분석 목적에 따라 검정 방법이 달라진다.
- 평균 비교, 독립성 검정, 분산 분석, 회귀 분석 등 여러 유형이 있으며, 각 검정 방법은 특정한 질문을 해결하는 데 사용됩니다.
    
    
    | **검정 종류** | **설명** |
    | --- | --- |
    | **단일 표본 t-검정
    (One-Sample t-test)** | 한 그룹의 평균이 특정 값과 다른지 검정합니다.
    - `예:` "한 도시의 평균 키가 170cm와 다른가?" |
    | **독립 표본 t-검정
    (Independent t-test)** | 두 그룹의 평균 차이를 검정합니다.
    - `예:` "남성과 여성의 평균 체온이 같은가?" |
    | **대응 표본 t-검정
    (Paired t-test)** | 동일한 그룹의 전/후 변화를 비교합니다.
    - `예:` "운동 프로그램 전후 체중 변화가 있는가?" |
    | **카이제곱 검정
    (Chi-Square Test)** | 범주형 변수 간 독립성을 검정합니다.
    - `예:` "흡연과 폐암 발병률 사이에 연관이 있는가?" |
    | **ANOVA
    (분산 분석, Analysis of Variance)** | 세 개 이상의 그룹 간 평균 차이를 검정합니다.
    - `예:` "세 개의 학급 간 시험 성적 차이가 존재하는가?" |
    | **회귀 분석
    (Regression Analysis)** | 한 변수(X)가 다른 변수(Y)에 미치는 영향을 검정합니다.
    - `예:` "광고비가 매출에 미치는 영향이 있는가?" |

### 사용 이유

- 보이는 차이가 우연이 아니라 통계적으로 유의미한 차이인지 검증하기 위해서 사용합니다.
- 가설 검정을 활용하면, 데이터 간 차이가 우연인지, 통계적으로 유의미한 차이인지를 수치적으로 검증할 수 있다.
    
    
    | **이유** | **설명** |
    | --- | --- |
    | **시각적 차이가 실제로 의미 있는지 검증하기 위해** | 그래프에서 특정 그룹 간 차이가 눈에 띄더라도, 이는 우연일 수도 있습니다. 
    설 검정을 수행하면, 해당 차이가 단순한 변동인지 아니면 통계적으로 유의미한 차이인지를 판단할 수 있습니다.
    예를 들어, A/B 테스트에서 두 광고의 클릭률 차이가 단순한 확률적 변동인지, 실제로 광고 효과가 다른 것인지 검증할 수 있습니다. |
    | **잘못된 결론을 방지하고 신뢰성 높은 분석을 위해** | 단순히 시각적으로 "차이가 있어 보인다"고 판단하는 것은 위험합니다.
    가설 검정을 통해 **p값, 검정 통계량, 신뢰 구간 등을 활용하여 객관적인 결론**을 도출할 수 있습니다.
    예를 들어, 신약 개발에서 약의 효과가 있어 보이더라도, 가설 검정을 통해 효과가 우연이 아닌지 수학적으로 검증해야 합니다. |
    | **데이터에 내재된 변동성을 고려하기 위해** | 모든 데이터는 어느 정도 변동성을 가지며, 같은 데이터라도 샘플을 다르게 추출하면 결과가 다르게 나타날 수 있습니다.
    가설 검정을 수행하면, 데이터 변동성을 반영하여 **우연한 변동인지, 실제 차이가 있는지 평가**할 수 있습니다.
    예를 들어, 주식 시장 데이터를 시각화할 때, 가격 변동이 단순한 시장 변동인지 특정 이벤트로 인한 차이인지 확인할 수 있습니다. |
    | **다양한 분석 방법과 함께 활용하기 위해** | 가설 검정은 평균 비교, 비율 비교, 분산 비교, 독립성 검정 등 다양한 방법과 함께 사용할 수 있습니다.
    예를 들어, 마케팅에서 A/B 테스트 결과를 시각화한 후, t-검정을 수행하여 두 집단 간 차이가 유의미한지 판단할 수 있습니다. |

## 통계적 시각화

### 정의

- 데이터의 분포, 관계, 추세 등을 효과적으로 분석하기 위해 통계적 기법을 활용하여 그래프나 차트로 표현하는 과정
- 통계적인 요약 및 분석을 포함하여 데이터의 패턴과 특성을 보다 깊이 있게 탐색하는 것이 목표

### 종류

- 단변량, 이변량, 다변량 분석으로 나뉜다.

| **구분** | **설명** | **예제 시각화** | 사용 예시 |
| --- | --- | --- | --- |
| **단변량 분석 (Univariate Analysis)** | 하나의 변수에 대한 데이터 분포 및 특성을 분석 | 히스토그램, 커널 밀도 추정(KDE), 박스플롯 | 학생들의 시험 점수 분포를 확인하기 위해 **히스토그램**을 사용 |
| **이변량 분석 (Bivariate Analysis)** | 두 변수 간의 관계를 분석 | 산점도, 상관 행렬, 회귀선 그래프 | 공부 시간과 성적 간 관계를 파악하기 위해 **산점도**를 사용 |
| **다변량 분석 (Multivariate Analysis)** | 세 개 이상의 변수 간 관계를 분석 | 페어플롯, 3D 산점도, 히트맵 | 여러 변수 간 연관성을 한눈에 보기 위해 **히트맵**을 사용 |

### 이유

- 데이터의 패턴과 분포를 직관적으로 파악하여 정확한 해석과 신뢰성 있는 분석을 하기 위해

| **이유** | **설명** |
| --- | --- |
| **데이터의 패턴을 쉽게 파악할 수 있음** | 통계적 시각화는 데이터의 전체적인 패턴과 경향성을 한눈에 보여줍니다. 예를 들어, 시계열 데이터의 경우 라인 차트를 통해 추세를 쉽게 확인할 수 있으며, 정규 분포 여부는 히스토그램이나 KDE 플롯을 통해 즉시 파악할 수 있습니다. |
| **데이터의 분포를 명확하게 분석 가능** | 데이터가 정규 분포를 따르는지, 치우쳐 있는지(왜도), 뾰족한 정도(첨도)를 분석할 수 있습니다. 이를 통해 적절한 통계적 분석 기법을 선택하는 데 도움이 됩니다. 예를 들어, t-검정이나 ANOVA와 같은 가설 검정 기법은 정규 분포 가정을 전제로 하기 때문에, 데이터 분포를 먼저 시각화하여 확인하는 과정이 필수적입니다. |
| **이상값(Outliers) 탐지가 용이함** | 박스플롯(Boxplot)을 활용하면 데이터에서 평균에서 크게 벗어난 이상값을 쉽게 발견할 수 있습니다. 이상값을 사전에 제거하거나 분석 과정에서 적절히 조정하면 더 신뢰성 높은 분석이 가능합니다. |
| **데이터 간 관계와 상관성을 쉽게 이해할 수 있음** | 두 변수 간의 관계를 분석할 때 산점도(Scatter Plot)나 상관행렬(Correlation Matrix) 같은 시각화를 활용하면 상관관계를 쉽게 확인할 수 있습니다. 예를 들어, 키와 몸무게 간의 관계를 시각화하면 두 변수가 양의 상관관계를 가지는지 여부를 직관적으로 알 수 있습니다. |
| **데이터의 변동성과 불확실성을 표현할 수 있음** | 선 그래프(Line Plot)와 신뢰 구간(Confidence Interval) 시각화를 통해 데이터의 변동성과 불확실성을 효과적으로 표현할 수 있습니다. 이는 특히 금융 데이터 분석, 예측 모델링, 실험 데이터 분석에서 중요한 역할을 합니다. |
| **데이터 비교 및 그룹 차이 분석이 가능함** | 여러 그룹 간의 차이를 비교할 때 막대 그래프(Bar Chart)나 바이올린 플롯(Violin Plot)을 활용하면 그룹 간의 평균 차이와 분포 차이를 동시에 분석할 수 있습니다. 예를 들어, 남성과 여성의 평균 키 차이를 막대 그래프로 비교하고, 분포 차이를 바이올린 플롯으로 추가적으로 분석할 수 있습니다. |
  
### 오늘의 회고
- 미니퀘스트 및 과제를 진행하자
- 도커 및 쿠버네티스 공부도 철저히 하자

